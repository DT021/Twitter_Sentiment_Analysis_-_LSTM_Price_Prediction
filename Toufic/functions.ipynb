{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import data.config as config\n",
    "import requests, time\n",
    "from pathlib import Path\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "BASE_URL = 'https://www.alphavantage.co/query?function='\n",
    "FUNCTION = 'TIME_SERIES_DAILY' #returns daily data vs. intraday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_closing_prices(symbols, output_size='full', name='stocks_history', api_key=config.ALPHA_VANTAGE_API): #symbol can either be a single ticker, or a list\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    symbols = symbols\n",
    "    output_size = output_size\n",
    "    for symbol in symbols:\n",
    "        # set request URL for GET request\n",
    "        request_URL = BASE_URL + \\\n",
    "f'{FUNCTION}&symbol={symbols}&outputsize={output_size}&apikey={api_key}'\n",
    "        # send GET request\n",
    "        r = requests.get(request_URL)\n",
    "        # create dataframe from GET response (transpose dataframe, as response flips\n",
    "        # rows and columns (dates are in columns instead of rows)\n",
    "        _ = pd.DataFrame(r.json()['Time Series (Daily)']).transpose()\n",
    "        # rename columns to include ticker\n",
    "        columns = [\n",
    "            f'{symbol}_open',\n",
    "            f'{symbol}_high',\n",
    "            f'{symbol}_low',\n",
    "            f'{symbol}_close',\n",
    "            f'{symbol}_volume'\n",
    "        ]\n",
    "        _.columns = columns\n",
    "        # join dataframe to \"master\" dataframe\n",
    "        df = df.join(_, how='outer')\n",
    "        time.sleep(15) #sleeping due to call limit on api (5 calls per minute)\n",
    "        # sort index in ascending order\n",
    "        df.sort_index(ascending=True, inplace=True)\n",
    "        df.to_csv(f'{name}.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_data(df, window, feature_col_number, target_col_number):\n",
    "    \"\"\"\n",
    "    This function accepts the column number for the features (X) and the target (y).\n",
    "    It chunks the data up with a rolling window of Xt - window to predict Xt.\n",
    "    It returns two numpy arrays of X and y.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(df) - window - 1):\n",
    "        features = df.iloc[i : (i + window), feature_col_number]\n",
    "        target = df.iloc[(i + window), target_col_number]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X,y,split_pct=0.7):\n",
    "    \n",
    "    # Use percent (70% by default) of the data for training and the remainder for testing\n",
    "    split = int(split_pct * len(X))\n",
    "    X_train = X[: split - 1]\n",
    "    X_test = X[split:]\n",
    "    y_train = y[: split - 1]\n",
    "    y_test = y[split:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_lstm(df, feature_column, target_column, window_size, batch_size, dropout_fraction,\\\n",
    "             epoch, name='exported_1lstm_model', *args, **kwargs):\n",
    "    \n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    split_pct = kwargs.get('split_pct',0.7)\n",
    "    check_point = kwargs.get('check_point',None)\n",
    "    early_stop = kwargs.get('early_stop', None)\n",
    "    \n",
    "    X, y = window_data(df, window_size, feature_column, target_column)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,split_pct)\n",
    "\n",
    "    # Use the MinMaxScaler to scale data between 0 and 1.\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    scaler.fit(y)\n",
    "    y_train = scaler.transform(y_train)\n",
    "    y_test = scaler.transform(y_test)\n",
    "\n",
    "    # Reshape the features for the model\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Define the LSTM RNN model.\n",
    "    model = Sequential()\n",
    "    # Layer 1\n",
    "    model.add(LSTM(\n",
    "        units=window_size,\n",
    "        input_shape=(X_train.shape[1], 1))\n",
    "        )\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    # Train the model\n",
    "    if check_point != None and early_stop != None:\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epoch,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks = [check_point, early_stop]\n",
    "        )\n",
    "    else:\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epoch,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        model.save(f'{name}.h5')\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # Make some predictions\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "    # Recover the original prices instead of the scaled version\n",
    "    predicted_prices = scaler.inverse_transform(predicted)\n",
    "    real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Create a DataFrame of Real and Predicted values\n",
    "    stocks = pd.DataFrame({\n",
    "        \"Real\": real_prices.ravel(),\n",
    "        \"Predicted\": predicted_prices.ravel()\n",
    "    })\n",
    "    \n",
    "#     model_json = model.to_json()\n",
    "#     with open(f'{name}.json', 'w') as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     model.save_weights(f'{name}.h5')\n",
    "    \n",
    "    return model, loss, stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_lstm(df, feature_column, target_column, window_size, batch_size, dropout_fraction,\\\n",
    "             epoch, split_pct, name='exported_2lstm_model', *args, **kwargs):\n",
    "    \n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    split_pct = kwargs.get('split_pct',0.7)\n",
    "    check_point = kwargs.get('check_point',None)\n",
    "    early_stop = kwargs.get('early_stop', None)\n",
    "    \n",
    "    X, y = window_data(df, window_size, feature_column, target_column)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,split_pct)\n",
    "\n",
    "    # Use the MinMaxScaler to scale data between 0 and 1.\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    scaler.fit(y)\n",
    "    y_train = scaler.transform(y_train)\n",
    "    y_test = scaler.transform(y_test)\n",
    "\n",
    "    # Reshape the features for the model\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    # Define the LSTM RNN model.\n",
    "    model = Sequential()\n",
    "    # Layer 1\n",
    "    model.add(LSTM(\n",
    "        units=window_size,\n",
    "        return_sequences=True,\n",
    "        input_shape=(X_train.shape[1], 1))\n",
    "        )\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "    # Layer 2\n",
    "    model.add(LSTM(units=row['window size']))\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "    # Train the model\n",
    "    if check_point != None and early_stop != None:\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epoch,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks = [check_point, early_stop]\n",
    "        )\n",
    "    else:\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epoch,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        model.save(f'{name}.h5')\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    # Make some predictions\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "    # Recover the original prices instead of the scaled version\n",
    "    predicted_prices = scaler.inverse_transform(predicted)\n",
    "    real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Create a DataFrame of Real and Predicted values\n",
    "    stocks = pd.DataFrame({\n",
    "        \"Real\": real_prices.ravel(),\n",
    "        \"Predicted\": predicted_prices.ravel()\n",
    "    })\n",
    "    \n",
    "#     model_json = model.to_json()\n",
    "#     with open(f'{name}.json', 'w') as json_file:\n",
    "#         json_file.write(model_json)\n",
    "#     model.save_weights(f'{name}.h5')\n",
    "    \n",
    "    return model, loss, stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_reshape_data(X, y=None):\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X)\n",
    "    X = scaler.transform(X)\n",
    "    if y != None:\n",
    "        scaler.fit(y)\n",
    "        y = scaler.transform(y)\n",
    "        X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "        return X, y\n",
    "    \n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
